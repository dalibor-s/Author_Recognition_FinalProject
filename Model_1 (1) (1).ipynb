{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJao1Rrq4HNK"
   },
   "source": [
    "Our goal is to create an NLP classifier that when given a paragraph from a famous classical book will be able to predict the text's author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tDTLQBNuSbxU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZG9TGZr9DlF"
   },
   "source": [
    "# Creating the feature set and label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPF1fCYNs-lK",
    "outputId": "918dcb5e-ea0c-4936-a165-278bcfcd6b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Pretty soon I wanted to smoke, and asked the w...\n",
       "1      Her sister, Miss Watson, a tolerable slim old ...\n",
       "2      Now she had got a start, and she went on and t...\n",
       "3      Miss Watson she kept pecking at me, and it got...\n",
       "4      I set down again, a-shaking all over, and got ...\n",
       "                             ...                        \n",
       "994    I was on the point of asking him what that wor...\n",
       "995      1. Knowledge of Literature.--Nil.\\n  2.     ...\n",
       "996    I see that I have alluded above to his powers ...\n",
       "997    During the first week or so we had no callers,...\n",
       "998    It was upon the 4th of March, as I have good r...\n",
       "Name: v2, Length: 999, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_text = 'books_and_authors'\n",
    "data = pd.read_csv(path_to_text, names=['v1', 'v2'])\n",
    "\n",
    "label = data['v1']\n",
    "text = data['v2']\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HM5b5MXpjwu"
   },
   "source": [
    "# Text preprocessing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQMEPwLuVos_",
    "outputId": "dd8d0c86-8760-4c0d-fbb8-4bd40ee6464b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/kicho/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kicho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kicho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kicho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kicho/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3msY8Crf2vm5"
   },
   "source": [
    "We'll try two different datasets. The first (text_preprocessed) will be fully preprocessed (low letters, missing punctuation, words represented like tokens and then lemmatization. The second dataset (text_preprocessed_1) will only have low letters and will be tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyfDsCg15M20"
   },
   "source": [
    "#First dataset - fully preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G4TuGNFyVFld"
   },
   "outputs": [],
   "source": [
    "text_preprocessed = []\n",
    "for sentence in text:\n",
    "    #sentence lower\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #string punct\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # stop-words\n",
    "    tokens_stop_words = []\n",
    "    \n",
    "    for token in tokens:\n",
    "          if token not in stopwords_:\n",
    "                tokens_stop_words.append(token)\n",
    "                \n",
    "    # Lemmatization\n",
    "    tokens_lemma = []\n",
    "    for token in tokens_stop_words:\n",
    "          tokens_lemma.append(wnl.lemmatize(token, get_wordnet_pos(nltk.pos_tag([token])[0][1])))\n",
    "            \n",
    "    final = ' '.join(tokens_lemma)\n",
    "    \n",
    "    text_preprocessed.append(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCSyKfMSXUV-"
   },
   "source": [
    "#Second dataset with lower letters and tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S1lVUTtWS2sW"
   },
   "outputs": [],
   "source": [
    "text_preprocessed_1 = []\n",
    "for sentence in text:\n",
    "    #sentence lower\n",
    "    sentence = sentence.lower()\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    text_preprocessed_1.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToIgCuOZtlb8",
    "outputId": "2c63193d-75c4-48f4-b5bc-9d5c63d1d1f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 999, 999)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_preprocessed_1),len(text_preprocessed), len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf-0K8WSt-Vq"
   },
   "source": [
    "### Extracting Text and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV50_5d7t9Pe",
    "outputId": "26a8d177-831c-4474-a5f8-850ddffae1e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    text_preprocessed, label, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsBvpyLw5oAn"
   },
   "source": [
    "Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRaf0P1JuHBa",
    "outputId": "77b3ad31-c8fc-4e54-fa41-a5120899fad4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "trainY= le.fit_transform(trainY)\n",
    "testY = le.fit_transform(testY)\n",
    "trainY.shape,testY.shape\n",
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROCu1wnCpXiQ",
    "outputId": "244d3a63-3daa-452c-aed8-8797ac5a414d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]), array([ 68, 104,  89, 167,  74, 108,  89]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trainY, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DlRHzeJspXiR"
   },
   "outputs": [],
   "source": [
    "#trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Oryyg39-MTA"
   },
   "source": [
    "First, we'll use CountVectorizer to process the data and to show us the number of apperances of each token (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8Y0uHPguNI1",
    "outputId": "84cd9426-38b3-447f-91fb-0b2d8d86e963"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(1, 1), max_features=5000)\n",
    "count_vect.fit(text_preprocessed)\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "trainX_vec = count_vect.transform(trainX)\n",
    "testX_vec = count_vect.transform(testX)\n",
    "trainX_vec.shape,\n",
    "testX_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p1GrRQdJOkzm"
   },
   "outputs": [],
   "source": [
    "dict_ = count_vect.vocabulary_\n",
    "new_data = pd.DataFrame.from_dict(dict_, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "pEmOBj94Ru9Z",
    "outputId": "29de336e-b63b-461e-a675-42dfe04cf348"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pretty</th>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soon</th>\n",
       "      <td>4202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>4817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoke</th>\n",
       "      <td>4162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moorgate</th>\n",
       "      <td>2852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coroner</th>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stamford</th>\n",
       "      <td>4269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>4486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "pretty      3526\n",
       "soon        4202\n",
       "want        4817\n",
       "smoke       4162\n",
       "ask          274\n",
       "...          ...\n",
       "moorgate    2852\n",
       "coroner      970\n",
       "stamford    4269\n",
       "laboratory  2404\n",
       "test        4486\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9S5h4vZnq6H",
    "outputId": "5d5d1664-835b-416d-8102-8a21caed06cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_['crime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2wb9TqQuYZE",
    "outputId": "f29c52bd-9993-4ab7-a31a-b8e1eaa92b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83        30\n",
      "           1       0.83      0.87      0.85        39\n",
      "           2       0.70      0.74      0.72        31\n",
      "           3       0.83      0.95      0.89        65\n",
      "           4       0.93      0.87      0.90        46\n",
      "           5       0.76      0.71      0.73        48\n",
      "           6       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.83      0.81      0.82       300\n",
      "weighted avg       0.83      0.83      0.83       300\n",
      "\n",
      "[[24  1  4  0  0  1  0]\n",
      " [ 1 34  0  3  1  0  0]\n",
      " [ 1  1 23  1  0  5  0]\n",
      " [ 0  2  0 62  0  1  0]\n",
      " [ 1  0  0  2 40  0  3]\n",
      " [ 1  1  5  5  1 34  1]\n",
      " [ 0  2  1  2  1  4 31]]\n",
      "Accuracy: 0.8266666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "#create an instance of the model\n",
    "lr_model = LogisticRegression(random_state=7, C=1, max_iter = 500) #pomalo C se poloshi, pogolemo isto poloshi\n",
    "#train the model\n",
    "lr_model.fit(trainX_vec, trainY)\n",
    "\n",
    "#predict test data\n",
    "pred_test = lr_model.predict(testX_vec)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test))\n",
    "print(confusion_matrix(testY,pred_test))\n",
    "print(\"Accuracy:\", accuracy_score(testY, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4KSr1-hn4aY"
   },
   "source": [
    "### Вториот модел - не процесиран текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxSTEdkbx9sZ",
    "outputId": "8441a502-7177-49cd-8836-f030f84cce3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX_1, testX_1, trainY_1, testY_1 = train_test_split(\n",
    "    text_preprocessed_1, label, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)\n",
    "\n",
    "\n",
    "len(trainX_1), len(testX_1) ,len(trainY_1), len(testY_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO-aOegA1S4k",
    "outputId": "ee78b66b-423a-415b-f9d4-8d330924131c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "trainY_1= le.fit_transform(trainY_1)\n",
    "testY_1 = le.fit_transform(testY_1)\n",
    "trainY_1.shape,testY_1.shape\n",
    "trainY_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CT0m5SOw1sOL",
    "outputId": "cbf88798-864c-4e96-adf6-556a317f565a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_1 = CountVectorizer(max_features=5000)\n",
    "count_vect_1.fit(text_preprocessed_1)\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "trainX_1_vec = count_vect.transform(trainX_1)\n",
    "testX_1_vec = count_vect.transform(testX_1)\n",
    "trainX_1_vec.shape,\n",
    "testX_1_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1bUVcdYo-iF",
    "outputId": "cbbf2a36-941c-43f6-ed89-a8ff64199ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        30\n",
      "           1       0.76      0.90      0.82        39\n",
      "           2       0.76      0.71      0.73        31\n",
      "           3       0.81      0.94      0.87        65\n",
      "           4       0.84      0.78      0.81        46\n",
      "           5       0.85      0.73      0.79        48\n",
      "           6       0.79      0.66      0.72        41\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.80      0.80      0.80       300\n",
      "weighted avg       0.81      0.81      0.80       300\n",
      "\n",
      "[[26  1  1  1  1  0  0]\n",
      " [ 1 35  0  0  2  0  1]\n",
      " [ 2  1 22  1  1  3  1]\n",
      " [ 0  2  0 61  1  1  0]\n",
      " [ 2  2  0  2 36  0  4]\n",
      " [ 0  2  4  6  0 35  1]\n",
      " [ 1  3  2  4  2  2 27]]\n",
      "Accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=0, C=100, max_iter=1000) #so ponisko C, polosha preciznost\n",
    "lr_model_1.fit(trainX_1_vec, trainY_1)\n",
    "\n",
    "pred_test_1 = lr_model_1.predict(testX_1_vec)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY_1,pred_test_1))\n",
    "print(confusion_matrix(testY_1,pred_test_1))\n",
    "print(\"Accuracy:\", accuracy_score(testY_1, pred_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mYdUrcYmS5wt"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDLweyMdqpwX",
    "outputId": "d5085b34-5141-4093-8331-a1c997bfcc08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((699, 5000), (300, 5000))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words={\"english\"}, ngram_range=(1, 3)) \n",
    "tfidf.fit(text)\n",
    "\n",
    "X_train_tfidf = tfidf.transform(trainX)\n",
    "X_test_tfidf = tfidf.transform(testX)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfPZHh8lxw1z",
    "outputId": "c50b675d-21f8-47fc-a90c-0d9458fb4932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83        30\n",
      "           1       0.82      0.92      0.87        39\n",
      "           2       0.75      0.77      0.76        31\n",
      "           3       0.91      0.98      0.95        65\n",
      "           4       0.93      0.91      0.92        46\n",
      "           5       0.89      0.85      0.87        48\n",
      "           6       0.85      0.83      0.84        41\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.87      0.86      0.86       300\n",
      "weighted avg       0.88      0.88      0.88       300\n",
      "\n",
      "[[22  2  3  2  0  1  0]\n",
      " [ 0 36  0  0  1  0  2]\n",
      " [ 1  1 24  1  0  3  1]\n",
      " [ 0  1  0 64  0  0  0]\n",
      " [ 0  0  0  1 42  0  3]\n",
      " [ 0  1  3  2  1 41  0]\n",
      " [ 0  3  2  0  1  1 34]]\n",
      "Accuracy: 0.8766666666666667\n"
     ]
    }
   ],
   "source": [
    "lr_model_tf = LogisticRegression(random_state=0, C=10, max_iter=1000)\n",
    "lr_model_tf.fit(X_train_tfidf, trainY)\n",
    "pred_test_tf = lr_model_tf.predict(X_test_tfidf)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test_tf))\n",
    "print(confusion_matrix(testY,pred_test_tf))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnJbRVRMEyLb"
   },
   "source": [
    "#Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xPGjahKHE2sS"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBLDOoiqFTab",
    "outputId": "871a5fbe-b662-4e47-ccfe-224f1e60fd45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.70        30\n",
      "           1       0.85      0.90      0.88        39\n",
      "           2       0.88      0.71      0.79        31\n",
      "           3       0.74      0.97      0.84        65\n",
      "           4       0.98      0.91      0.94        46\n",
      "           5       0.75      0.81      0.78        48\n",
      "           6       0.84      0.78      0.81        41\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.86      0.80      0.82       300\n",
      "weighted avg       0.85      0.83      0.83       300\n",
      "\n",
      "[[16  3  2  6  0  2  1]\n",
      " [ 0 35  0  1  0  0  3]\n",
      " [ 0  1 22  3  1  4  0]\n",
      " [ 0  1  1 63  0  0  0]\n",
      " [ 0  0  0  1 42  2  1]\n",
      " [ 0  1  0  7  0 39  1]\n",
      " [ 0  0  0  4  0  5 32]]\n",
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=0.01)\n",
    "nb.fit(X_train_tfidf, trainY)\n",
    "pred_test_nb = nb.predict(X_test_tfidf)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test_nb))\n",
    "print(confusion_matrix(testY,pred_test_nb))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBIdX8qHDYA"
   },
   "source": [
    "#SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7esh_FXJRJR",
    "outputId": "4dd7053a-1ac4-4c0e-80cf-164425900af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77        30\n",
      "           1       0.74      0.87      0.80        39\n",
      "           2       0.70      0.74      0.72        31\n",
      "           3       0.84      0.88      0.86        65\n",
      "           4       0.93      0.80      0.86        46\n",
      "           5       0.63      0.71      0.67        48\n",
      "           6       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.79      0.78      0.78       300\n",
      "weighted avg       0.80      0.79      0.79       300\n",
      "\n",
      "[[22  1  2  0  0  5  0]\n",
      " [ 1 34  1  2  1  0  0]\n",
      " [ 1  1 23  1  0  5  0]\n",
      " [ 0  3  1 57  0  3  1]\n",
      " [ 1  2  0  2 37  3  1]\n",
      " [ 2  3  3  4  1 34  1]\n",
      " [ 0  2  3  2  1  4 29]]\n",
      "Accuracy: 0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC(kernel='linear', probability=True, C=10, gamma=0.1)\n",
    "svm_model.fit(trainX_vec, trainY)\n",
    "\n",
    "pred_test_svm = svm_model.predict(testX_vec)\n",
    "print(classification_report(testY,pred_test_svm))\n",
    "print(confusion_matrix(testY,pred_test_svm))\n",
    "print(\"Accuracy:\", accuracy_score(testY, pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9YKtB8-pXid",
    "outputId": "c70397c1-dc72-4ec8-8330-072101032393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Now she had got a start, and she went on and told me all about the good\n",
      "place. She said all a body would have to do there was to go around all\n",
      "day long with a harp and sing, forever and ever. So I didn't think\n",
      "much of it. But I never said so. I asked her if she reckoned Tom Sawyer\n",
      "would go there, and she said not by a considerable sight. I was glad\n",
      "about that, because I wanted him and me to be together.\n"
     ]
    }
   ],
   "source": [
    "words= nltk.word_tokenize(text[2])\n",
    "length= len(words) \n",
    "print(length) \n",
    "print(text[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bX8a2DtpXid",
    "outputId": "11992b63-6ee8-486a-f5c1-15122fd02e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Yo' ole father doan' know yit what he's a-gwyne to do. Sometimes he\n",
      "spec he'll go 'way, en den agin he spec he'll stay. De bes' way is to\n",
      "res' easy en let de ole man take his own way. Dey's two angels hoverin'\n",
      "roun' 'bout him. One uv 'em is white en shiny, en t'other one is black.\n",
      "De white one gits him to go right a little while, den de black one sail\n",
      "in en bust it all up. A body can't tell yit which one gwyne to fetch\n",
      "him at de las'. But you is all right. You gwyne to have considable\n",
      "trouble in yo' life, en considable joy. Sometimes you gwyne to git\n",
      "hurt, en sometimes you gwyne to git sick; but every time you's gwyne\n",
      "to git well agin. Dey's two gals flyin' 'bout you in yo' life. One\n",
      "uv 'em's light en t'other one is dark. One is rich en t'other is po'.\n",
      "You's gwyne to marry de po' one fust en de rich one by en by. You\n",
      "wants to keep 'way fum de water as much as you kin, en don't run no\n",
      "resk, 'kase it's down in de bills dat you's gwyne to git hung.”\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "paragraph = text[25]\n",
    "sentences = nltk.sent_tokenize(paragraph) \n",
    "length= len(sentences) \n",
    "print(text[25]) \n",
    "print(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HzUSRQfOZylr"
   },
   "outputs": [],
   "source": [
    "# # embeddings \n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from gensim.models import Phrases\n",
    "\n",
    "# bigramer = Phrases(sentences)\n",
    "# model = Word2Vec(bigramer[sentences], window=5, min_count=10, workers=4)\n",
    "\n",
    "# # unload memory\n",
    "# model.init_sims(replace=True) \n",
    "\n",
    "# # Storing a model\n",
    "# model.save(\"author\")\n",
    "# # new_model = gensim.models.Word2Vec.load('author')\n",
    "\n",
    "# # Switch to KeyedVectors instance  \n",
    "# # w2v = {w: vec for w,vec in text_preprocessed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dvXe3ijQehQm"
   },
   "outputs": [],
   "source": [
    "# model.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nae2IGVjicsO"
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    # If word2vec were passed in during initialization, use those\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 100\n",
    "    \n",
    "    # learning word2weight\n",
    "    def fit(self, X, y):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
    "        vect.fit(X)\n",
    "        max_idf = max(vect.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "        lambda: max_idf, [(w, vect.idf_[i]) for w, i in vect.vocabulary_.items()]\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    # Use learned word2weight\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([\n",
    "                self.word2vec[w]*self.word2weight[w] \n",
    "                for w in words if w in self.word2vec] or \n",
    "                [np.zeros(self.dim)], axis=0) \n",
    "            for words in X\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8QH12lzY_W_Z"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer #similar to the CountVectorizer and TfIDF from sci-kit\n",
    "\n",
    "#The word embedding layer expects input sequences to be comprised of integers.\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(text_preprocessed)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3ES4MU5_0jI",
    "outputId": "124872f1-5732-4682-fa2f-d7e91598fce6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'’': 1,\n",
       " '“': 2,\n",
       " '”': 3,\n",
       " 'say': 4,\n",
       " 'would': 5,\n",
       " 'one': 6,\n",
       " 'go': 7,\n",
       " 'mr': 8,\n",
       " 'make': 9,\n",
       " 'man': 10,\n",
       " 'could': 11,\n",
       " 'like': 12,\n",
       " 'come': 13,\n",
       " 'take': 14,\n",
       " 'time': 15,\n",
       " 'get': 16,\n",
       " 'see': 17,\n",
       " 'upon': 18,\n",
       " 'little': 19,\n",
       " 'know': 20,\n",
       " 'look': 21,\n",
       " 'well': 22,\n",
       " 'great': 23,\n",
       " 'hand': 24,\n",
       " 'good': 25,\n",
       " 'give': 26,\n",
       " 'much': 27,\n",
       " 'thing': 28,\n",
       " 'seem': 29,\n",
       " 'way': 30,\n",
       " 'old': 31,\n",
       " '‘': 32,\n",
       " 'might': 33,\n",
       " 'day': 34,\n",
       " 'never': 35,\n",
       " 'even': 36,\n",
       " 'two': 37,\n",
       " 'every': 38,\n",
       " 'eye': 39,\n",
       " 'men': 40,\n",
       " 'turn': 41,\n",
       " 'life': 42,\n",
       " 'head': 43,\n",
       " 'thought': 44,\n",
       " 'house': 45,\n",
       " 'sir': 46,\n",
       " 'back': 47,\n",
       " 'young': 48,\n",
       " 'people': 49,\n",
       " 'woman': 50,\n",
       " 'first': 51,\n",
       " 'work': 52,\n",
       " 'tom': 53,\n",
       " 'must': 54,\n",
       " 'think': 55,\n",
       " 'may': 56,\n",
       " 'mind': 57,\n",
       " 'always': 58,\n",
       " 'many': 59,\n",
       " 'call': 60,\n",
       " 'face': 61,\n",
       " 'saw': 62,\n",
       " 'away': 63,\n",
       " 'long': 64,\n",
       " 'though': 65,\n",
       " 'u': 66,\n",
       " 'last': 67,\n",
       " 'year': 68,\n",
       " 'without': 69,\n",
       " 'want': 70,\n",
       " 'nothing': 71,\n",
       " 'ever': 72,\n",
       " 'casaubon': 73,\n",
       " 'boy': 74,\n",
       " 'night': 75,\n",
       " 'put': 76,\n",
       " 'place': 77,\n",
       " 'world': 78,\n",
       " 'yet': 79,\n",
       " 'found': 80,\n",
       " 'lady': 81,\n",
       " 'knew': 82,\n",
       " 'right': 83,\n",
       " 'open': 84,\n",
       " 'friend': 85,\n",
       " 'enough': 86,\n",
       " 'three': 87,\n",
       " 'large': 88,\n",
       " 'still': 89,\n",
       " 'miss': 90,\n",
       " 'begin': 91,\n",
       " 'point': 92,\n",
       " 'small': 93,\n",
       " 'show': 94,\n",
       " 'mother': 95,\n",
       " 'joe': 96,\n",
       " 'soon': 97,\n",
       " 'tell': 98,\n",
       " 'light': 99,\n",
       " 'end': 100,\n",
       " 'side': 101,\n",
       " 'word': 102,\n",
       " 'far': 103,\n",
       " 'water': 104,\n",
       " 'fire': 105,\n",
       " 'another': 106,\n",
       " 'quite': 107,\n",
       " 'ask': 108,\n",
       " 'use': 109,\n",
       " 'keep': 110,\n",
       " 'home': 111,\n",
       " 'part': 112,\n",
       " 'new': 113,\n",
       " 'round': 114,\n",
       " 'family': 115,\n",
       " 'child': 116,\n",
       " 'left': 117,\n",
       " 'shall': 118,\n",
       " 'dorothea': 119,\n",
       " 'dog': 120,\n",
       " 'brooke': 121,\n",
       " 'along': 122,\n",
       " 'sort': 123,\n",
       " 'however': 124,\n",
       " 'letter': 125,\n",
       " 'sister': 126,\n",
       " 'feel': 127,\n",
       " 'heart': 128,\n",
       " 'set': 129,\n",
       " 'lay': 130,\n",
       " 'do': 131,\n",
       " 'black': 132,\n",
       " 'rather': 133,\n",
       " 'kind': 134,\n",
       " 'talk': 135,\n",
       " 'poor': 136,\n",
       " 'door': 137,\n",
       " 'among': 138,\n",
       " 'love': 139,\n",
       " 'heard': 140,\n",
       " 'let': 141,\n",
       " 'find': 142,\n",
       " 'whole': 143,\n",
       " 'till': 144,\n",
       " 'high': 145,\n",
       " 'moment': 146,\n",
       " 'become': 147,\n",
       " 'something': 148,\n",
       " 'felt': 149,\n",
       " 'close': 150,\n",
       " 'walk': 151,\n",
       " 'maggie': 152,\n",
       " 'mean': 153,\n",
       " 'anything': 154,\n",
       " 'money': 155,\n",
       " 'believe': 156,\n",
       " 'interest': 157,\n",
       " 'wish': 158,\n",
       " 'whose': 159,\n",
       " 'live': 160,\n",
       " 'stand': 161,\n",
       " 'present': 162,\n",
       " 'course': 163,\n",
       " 'foot': 164,\n",
       " 'body': 165,\n",
       " 'towards': 166,\n",
       " 'air': 167,\n",
       " 'try': 168,\n",
       " 'bad': 169,\n",
       " 'next': 170,\n",
       " 'learn': 171,\n",
       " 'form': 172,\n",
       " 'hour': 173,\n",
       " 'room': 174,\n",
       " 'nature': 175,\n",
       " 'around': 176,\n",
       " 'best': 177,\n",
       " 'father': 178,\n",
       " 'carry': 179,\n",
       " 'wife': 180,\n",
       " 'told': 181,\n",
       " 'rest': 182,\n",
       " 'name': 183,\n",
       " 'behind': 184,\n",
       " 'half': 185,\n",
       " 'gentleman': 186,\n",
       " 'reason': 187,\n",
       " 'dear': 188,\n",
       " 'book': 189,\n",
       " 'celia': 190,\n",
       " 'together': 191,\n",
       " 'run': 192,\n",
       " 'minute': 193,\n",
       " 'country': 194,\n",
       " 'less': 195,\n",
       " 'manner': 196,\n",
       " 'also': 197,\n",
       " 'cold': 198,\n",
       " 'white': 199,\n",
       " 'speak': 200,\n",
       " 'perhaps': 201,\n",
       " 'dark': 202,\n",
       " 'morning': 203,\n",
       " 'omar': 204,\n",
       " 'tulliver': 205,\n",
       " 'die': 206,\n",
       " 'near': 207,\n",
       " 'certain': 208,\n",
       " 'fact': 209,\n",
       " 'hair': 210,\n",
       " 'write': 211,\n",
       " 'fine': 212,\n",
       " 'read': 213,\n",
       " 'full': 214,\n",
       " 'within': 215,\n",
       " 'idea': 216,\n",
       " 'pretty': 217,\n",
       " 'sure': 218,\n",
       " 'short': 219,\n",
       " 'appear': 220,\n",
       " 'girl': 221,\n",
       " 'brought': 222,\n",
       " 'church': 223,\n",
       " 'everything': 224,\n",
       " 'order': 225,\n",
       " 'help': 226,\n",
       " 'step': 227,\n",
       " 'sat': 228,\n",
       " 'return': 229,\n",
       " 'often': 230,\n",
       " 'poe': 231,\n",
       " 'window': 232,\n",
       " 'since': 233,\n",
       " 'follow': 234,\n",
       " 'matter': 235,\n",
       " 'held': 236,\n",
       " 'leave': 237,\n",
       " 'really': 238,\n",
       " 'indeed': 239,\n",
       " 'stood': 240,\n",
       " 'cry': 241,\n",
       " 'death': 242,\n",
       " 'leg': 243,\n",
       " 'stop': 244,\n",
       " 'arm': 245,\n",
       " 'whether': 246,\n",
       " 'person': 247,\n",
       " 'wolf': 248,\n",
       " 'knowledge': 249,\n",
       " 'several': 250,\n",
       " 'didnt': 251,\n",
       " 'mouth': 252,\n",
       " 'rise': 253,\n",
       " 'move': 254,\n",
       " 'hope': 255,\n",
       " 'almost': 256,\n",
       " 'master': 257,\n",
       " 'table': 258,\n",
       " 'sound': 259,\n",
       " 'notice': 260,\n",
       " 'mention': 261,\n",
       " 'hold': 262,\n",
       " 'understand': 263,\n",
       " 'ran': 264,\n",
       " 'remember': 265,\n",
       " 'voice': 266,\n",
       " 'character': 267,\n",
       " 'dead': 268,\n",
       " 'six': 269,\n",
       " 'king': 270,\n",
       " 'opinion': 271,\n",
       " 'wind': 272,\n",
       " 'need': 273,\n",
       " 'town': 274,\n",
       " 'hear': 275,\n",
       " 'alone': 276,\n",
       " 'beautiful': 277,\n",
       " 'line': 278,\n",
       " 'across': 279,\n",
       " 'street': 280,\n",
       " 'age': 281,\n",
       " 'regard': 282,\n",
       " 'least': 283,\n",
       " 'case': 284,\n",
       " 'english': 285,\n",
       " 'james': 286,\n",
       " 'particular': 287,\n",
       " 'start': 288,\n",
       " 'thousand': 289,\n",
       " 'five': 290,\n",
       " 'cut': 291,\n",
       " 'hardly': 292,\n",
       " 'hundred': 293,\n",
       " 'low': 294,\n",
       " 'occasion': 295,\n",
       " 'hard': 296,\n",
       " 'change': 297,\n",
       " 'kept': 298,\n",
       " 'lose': 299,\n",
       " 'ground': 300,\n",
       " 'state': 301,\n",
       " 'dress': 302,\n",
       " 'attention': 303,\n",
       " 'doubt': 304,\n",
       " 'sit': 305,\n",
       " 'view': 306,\n",
       " 'expect': 307,\n",
       " 'judge': 308,\n",
       " 'son': 309,\n",
       " 'general': 310,\n",
       " 'thus': 311,\n",
       " 'enter': 312,\n",
       " 'mere': 313,\n",
       " 'object': 314,\n",
       " 'receive': 315,\n",
       " 'silas': 316,\n",
       " 'heavy': 317,\n",
       " 'four': 318,\n",
       " 'wall': 319,\n",
       " 'sometimes': 320,\n",
       " 'drop': 321,\n",
       " 'school': 322,\n",
       " 'reach': 323,\n",
       " 'business': 324,\n",
       " 'boat': 325,\n",
       " 'brother': 326,\n",
       " 'fear': 327,\n",
       " 'question': 328,\n",
       " 'suppose': 329,\n",
       " 'send': 330,\n",
       " 'certainly': 331,\n",
       " 'couldnt': 332,\n",
       " 'spirit': 333,\n",
       " 'stone': 334,\n",
       " 'beyond': 335,\n",
       " 'remain': 336,\n",
       " 'fellow': 337,\n",
       " 'john': 338,\n",
       " 'clear': 339,\n",
       " 'stick': 340,\n",
       " 'rush': 341,\n",
       " 'lie': 342,\n",
       " 'bit': 343,\n",
       " 'laugh': 344,\n",
       " 'strong': 345,\n",
       " 'lead': 346,\n",
       " 'true': 347,\n",
       " 'forward': 348,\n",
       " 'horse': 349,\n",
       " 'already': 350,\n",
       " 'sense': 351,\n",
       " 'sea': 352,\n",
       " 'handsome': 353,\n",
       " 'dashwood': 354,\n",
       " 'marner': 355,\n",
       " 'listen': 356,\n",
       " 'big': 357,\n",
       " 'touch': 358,\n",
       " 'none': 359,\n",
       " 'front': 360,\n",
       " 'save': 361,\n",
       " 'pass': 362,\n",
       " 'effect': 363,\n",
       " 'truth': 364,\n",
       " 'expression': 365,\n",
       " 'husband': 366,\n",
       " 'trouble': 367,\n",
       " 'shoulder': 368,\n",
       " 'sign': 369,\n",
       " 'fell': 370,\n",
       " 'sleep': 371,\n",
       " 'happen': 372,\n",
       " 'eat': 373,\n",
       " 'else': 374,\n",
       " 'others': 375,\n",
       " 'bring': 376,\n",
       " 'grow': 377,\n",
       " 'possible': 378,\n",
       " 'uncle': 379,\n",
       " 'real': 380,\n",
       " 'cause': 381,\n",
       " 'ear': 382,\n",
       " 'hill': 383,\n",
       " 'charm': 384,\n",
       " 'month': 385,\n",
       " 'daughter': 386,\n",
       " 'fall': 387,\n",
       " 'watch': 388,\n",
       " 'soul': 389,\n",
       " 'neither': 390,\n",
       " 'land': 391,\n",
       " 'picture': 392,\n",
       " 'remarkable': 393,\n",
       " 'holmes': 394,\n",
       " 'company': 395,\n",
       " 'deep': 396,\n",
       " 'either': 397,\n",
       " 'met': 398,\n",
       " 'creature': 399,\n",
       " 'visit': 400,\n",
       " 'glance': 401,\n",
       " 'altogether': 402,\n",
       " 'answer': 403,\n",
       " 'minister': 404,\n",
       " 'add': 405,\n",
       " 'marriage': 406,\n",
       " 'power': 407,\n",
       " 'sight': 408,\n",
       " 'ready': 409,\n",
       " 'account': 410,\n",
       " 'offer': 411,\n",
       " 'wore': 412,\n",
       " 'god': 413,\n",
       " 'red': 414,\n",
       " 'except': 415,\n",
       " 'desire': 416,\n",
       " 'buck': 417,\n",
       " 'warnt': 418,\n",
       " 'everybody': 419,\n",
       " 'wait': 420,\n",
       " 'laid': 421,\n",
       " 'nose': 422,\n",
       " 'play': 423,\n",
       " 'wonder': 424,\n",
       " 'river': 425,\n",
       " 'throw': 426,\n",
       " 'corner': 427,\n",
       " 'peculiar': 428,\n",
       " 'wood': 429,\n",
       " 'pay': 430,\n",
       " 'strange': 431,\n",
       " 'village': 432,\n",
       " 'pull': 433,\n",
       " 'secret': 434,\n",
       " 'raise': 435,\n",
       " 'quiet': 436,\n",
       " 'direction': 437,\n",
       " 'ye': 438,\n",
       " 'early': 439,\n",
       " 'smile': 440,\n",
       " 'story': 441,\n",
       " 'appearance': 442,\n",
       " 'habit': 443,\n",
       " 'subject': 444,\n",
       " 'longer': 445,\n",
       " 'easy': 446,\n",
       " 'clothes': 447,\n",
       " 'hat': 448,\n",
       " 'narrow': 449,\n",
       " 'sent': 450,\n",
       " 'color': 451,\n",
       " 'court': 452,\n",
       " 'blue': 453,\n",
       " 'lip': 454,\n",
       " 'sun': 455,\n",
       " 'england': 456,\n",
       " 'therefore': 457,\n",
       " 'pain': 458,\n",
       " 'care': 459,\n",
       " 'beauty': 460,\n",
       " 'piece': 461,\n",
       " 'tree': 462,\n",
       " 'servant': 463,\n",
       " 'act': 464,\n",
       " 'em': 465,\n",
       " 'week': 466,\n",
       " 'chance': 467,\n",
       " 'study': 468,\n",
       " 'suffer': 469,\n",
       " 'produce': 470,\n",
       " 'spoke': 471,\n",
       " 'although': 472,\n",
       " 'toward': 473,\n",
       " 'purpose': 474,\n",
       " 'bread': 475,\n",
       " 'observe': 476,\n",
       " 'past': 477,\n",
       " 'affection': 478,\n",
       " 'chettam': 479,\n",
       " 'wouldnt': 480,\n",
       " 'knee': 481,\n",
       " 'ten': 482,\n",
       " 'seat': 483,\n",
       " 'paper': 484,\n",
       " 'fair': 485,\n",
       " 'arab': 486,\n",
       " 'sword': 487,\n",
       " 'crowd': 488,\n",
       " 'besides': 489,\n",
       " 'pleasure': 490,\n",
       " 'circle': 491,\n",
       " 'settle': 492,\n",
       " 'consider': 493,\n",
       " 'draw': 494,\n",
       " 'dream': 495,\n",
       " 'darkness': 496,\n",
       " 'struggle': 497,\n",
       " 'relation': 498,\n",
       " 'usual': 499,\n",
       " 'genius': 500,\n",
       " 'wine': 501,\n",
       " 'taste': 502,\n",
       " 'bob': 503,\n",
       " 'widow': 504,\n",
       " 'watson': 505,\n",
       " 'cross': 506,\n",
       " 'instead': 507,\n",
       " 'nearly': 508,\n",
       " 'lean': 509,\n",
       " 'square': 510,\n",
       " 'rag': 511,\n",
       " 'weather': 512,\n",
       " 'fish': 513,\n",
       " 'presently': 514,\n",
       " 'passion': 515,\n",
       " 'happy': 516,\n",
       " 'finger': 517,\n",
       " 'length': 518,\n",
       " 'tale': 519,\n",
       " 'knight': 520,\n",
       " 'fortune': 521,\n",
       " 'arthur': 522,\n",
       " 'figure': 523,\n",
       " 'society': 524,\n",
       " 'search': 525,\n",
       " 'conversation': 526,\n",
       " 'promise': 527,\n",
       " 'able': 528,\n",
       " 'trust': 529,\n",
       " 'companion': 530,\n",
       " 'cadwallader': 531,\n",
       " 'riley': 532,\n",
       " 'kill': 533,\n",
       " 'kitchen': 534,\n",
       " 'neck': 535,\n",
       " 'mine': 536,\n",
       " 'afterwards': 537,\n",
       " 'marry': 538,\n",
       " 'stay': 539,\n",
       " 'roll': 540,\n",
       " 'cabin': 541,\n",
       " 'allow': 542,\n",
       " 'speech': 543,\n",
       " 'bright': 544,\n",
       " 'struck': 545,\n",
       " 'human': 546,\n",
       " 'action': 547,\n",
       " 'service': 548,\n",
       " 'presence': 549,\n",
       " 'difficulty': 550,\n",
       " 'various': 551,\n",
       " 'express': 552,\n",
       " 'secure': 553,\n",
       " 'especially': 554,\n",
       " 'fit': 555,\n",
       " 'lord': 556,\n",
       " 'norland': 557,\n",
       " 'anybody': 558,\n",
       " 'advantage': 559,\n",
       " 'seven': 560,\n",
       " 'different': 561,\n",
       " 'teeth': 562,\n",
       " 'whatever': 563,\n",
       " 'floor': 564,\n",
       " 'oh': 565,\n",
       " 'wonderful': 566,\n",
       " 'road': 567,\n",
       " 'wild': 568,\n",
       " 'position': 569,\n",
       " 'drew': 570,\n",
       " 'late': 571,\n",
       " 'second': 572,\n",
       " 'effort': 573,\n",
       " 'increase': 574,\n",
       " 'simple': 575,\n",
       " 'merely': 576,\n",
       " 'charles': 577,\n",
       " 'feature': 578,\n",
       " 'strength': 579,\n",
       " 'snarl': 580,\n",
       " 'deck': 581,\n",
       " 'cottage': 582,\n",
       " 'fetch': 583,\n",
       " 'star': 584,\n",
       " 'jim': 585,\n",
       " 'hung': 586,\n",
       " 'edge': 587,\n",
       " 'passage': 588,\n",
       " 'mark': 589,\n",
       " 'rich': 590,\n",
       " 'soldier': 591,\n",
       " 'worth': 592,\n",
       " 'warm': 593,\n",
       " 'dinner': 594,\n",
       " 'couple': 595,\n",
       " 'cover': 596,\n",
       " 'necessary': 597,\n",
       " 'confess': 598,\n",
       " 'launcelot': 599,\n",
       " 'hall': 600,\n",
       " 'shape': 601,\n",
       " 'respect': 602,\n",
       " 'reply': 603,\n",
       " 'pair': 604,\n",
       " 'address': 605,\n",
       " 'lift': 606,\n",
       " 'note': 607,\n",
       " 'satisfaction': 608,\n",
       " 'mill': 609,\n",
       " 'stretch': 610,\n",
       " 'reckon': 611,\n",
       " 'garden': 612,\n",
       " 'comfortable': 613,\n",
       " 'top': 614,\n",
       " 'mile': 615,\n",
       " 'broad': 616,\n",
       " 'blood': 617,\n",
       " 'snow': 618,\n",
       " 'hang': 619,\n",
       " 'natural': 620,\n",
       " 'free': 621,\n",
       " 'fast': 622,\n",
       " 'strike': 623,\n",
       " 'finally': 624,\n",
       " 'pocket': 625,\n",
       " 'gate': 626,\n",
       " 'captain': 627,\n",
       " 'pleasant': 628,\n",
       " 'term': 629,\n",
       " 'affair': 630,\n",
       " 'comfort': 631,\n",
       " 'mystery': 632,\n",
       " 'hunger': 633,\n",
       " 'entirely': 634,\n",
       " 'scene': 635,\n",
       " 'later': 636,\n",
       " 'ordinary': 637,\n",
       " 'history': 638,\n",
       " 'measure': 639,\n",
       " 'slowly': 640,\n",
       " 'latter': 641,\n",
       " 'party': 642,\n",
       " 'deal': 643,\n",
       " 'william': 644,\n",
       " 'gordon': 645,\n",
       " 'future': 646,\n",
       " 'london': 647,\n",
       " 'style': 648,\n",
       " 'health': 649,\n",
       " 'grey': 650,\n",
       " 'neighbour': 651,\n",
       " 'wopsle': 652,\n",
       " 'baskerville': 653,\n",
       " 'bed': 654,\n",
       " 'chair': 655,\n",
       " 'likely': 656,\n",
       " 'yard': 657,\n",
       " 'nobody': 658,\n",
       " 'fill': 659,\n",
       " 'pas': 660,\n",
       " 'law': 661,\n",
       " 'fog': 662,\n",
       " 'fresh': 663,\n",
       " 'green': 664,\n",
       " 'difficult': 665,\n",
       " 'shot': 666,\n",
       " 'memory': 667,\n",
       " 'pause': 668,\n",
       " 'circumstance': 669,\n",
       " 'press': 670,\n",
       " 'earth': 671,\n",
       " 'pride': 672,\n",
       " 'engage': 673,\n",
       " 'experience': 674,\n",
       " 'terror': 675,\n",
       " 'duty': 676,\n",
       " 'event': 677,\n",
       " 'suddenly': 678,\n",
       " 'complete': 679,\n",
       " 'perfect': 680,\n",
       " 'enjoy': 681,\n",
       " 'poem': 682,\n",
       " 'imagination': 683,\n",
       " 'impression': 684,\n",
       " 'elinor': 685,\n",
       " 'austin': 686,\n",
       " 'quality': 687,\n",
       " 'rode': 688,\n",
       " 'pap': 689,\n",
       " 'bank': 690,\n",
       " 'ill': 691,\n",
       " 'broke': 692,\n",
       " 'knife': 693,\n",
       " 'wrong': 694,\n",
       " 'sudden': 695,\n",
       " 'instant': 696,\n",
       " 'pound': 697,\n",
       " 'threw': 698,\n",
       " 'nine': 699,\n",
       " 'plan': 700,\n",
       " 'imagine': 701,\n",
       " 'deliver': 702,\n",
       " 'curl': 703,\n",
       " 'delight': 704,\n",
       " 'verse': 705,\n",
       " 'fashion': 706,\n",
       " 'animal': 707,\n",
       " 'arrive': 708,\n",
       " 'remark': 709,\n",
       " 'force': 710,\n",
       " 'honor': 711,\n",
       " 'sprang': 712,\n",
       " 'cease': 713,\n",
       " 'prefect': 714,\n",
       " 'furniture': 715,\n",
       " 'fail': 716,\n",
       " 'instance': 717,\n",
       " 'apart': 718,\n",
       " 'concern': 719,\n",
       " 'attempt': 720,\n",
       " 'sailor': 721,\n",
       " 'kindness': 722,\n",
       " 'leap': 723,\n",
       " 'pack': 724,\n",
       " 'passenger': 725,\n",
       " 'squire': 726,\n",
       " 'duff': 727,\n",
       " 'raveloe': 728,\n",
       " 'candle': 729,\n",
       " 'noise': 730,\n",
       " 'devil': 731,\n",
       " 'grand': 732,\n",
       " 'living': 733,\n",
       " 'de': 734,\n",
       " 'spring': 735,\n",
       " 'hunt': 736,\n",
       " 'terrible': 737,\n",
       " 'probably': 738,\n",
       " 'curiosity': 739,\n",
       " 'dare': 740,\n",
       " 'flower': 741,\n",
       " 'ago': 742,\n",
       " 'accord': 743,\n",
       " 'admiration': 744,\n",
       " 'depart': 745,\n",
       " 'spot': 746,\n",
       " 'sharp': 747,\n",
       " 'evil': 748,\n",
       " 'queen': 749,\n",
       " 'faith': 750,\n",
       " 'american': 751,\n",
       " 'match': 752,\n",
       " 'reflect': 753,\n",
       " 'wide': 754,\n",
       " 'shock': 755,\n",
       " 'determine': 756,\n",
       " 'fancy': 757,\n",
       " 'continued': 758,\n",
       " 'shout': 759,\n",
       " 'condition': 760,\n",
       " 'result': 761,\n",
       " 'immediately': 762,\n",
       " 'odd': 763,\n",
       " 'poet': 764,\n",
       " 'religious': 765,\n",
       " 'possess': 766,\n",
       " 'married': 767,\n",
       " 'join': 768,\n",
       " 'rope': 769,\n",
       " 'chest': 770,\n",
       " 'brown': 771,\n",
       " 'clergyman': 772,\n",
       " 'amiable': 773,\n",
       " 'edward': 774,\n",
       " 'clean': 775,\n",
       " 'leaf': 776,\n",
       " 'tie': 777,\n",
       " 'tear': 778,\n",
       " 'inside': 779,\n",
       " 'guard': 780,\n",
       " 'breakfast': 781,\n",
       " 'quick': 782,\n",
       " 'en': 783,\n",
       " 'hurt': 784,\n",
       " 'shut': 785,\n",
       " 'breath': 786,\n",
       " 'coat': 787,\n",
       " 'win': 788,\n",
       " 'bow': 789,\n",
       " 'language': 790,\n",
       " 'heel': 791,\n",
       " 'caught': 792,\n",
       " 'outside': 793,\n",
       " 'aunt': 794,\n",
       " 'suit': 795,\n",
       " 'conduct': 796,\n",
       " 'perfectly': 797,\n",
       " 'unto': 798,\n",
       " 'feeling': 799,\n",
       " 'pity': 800,\n",
       " 'success': 801,\n",
       " 'remove': 802,\n",
       " 'sunday': 803,\n",
       " 'equal': 804,\n",
       " 'bound': 805,\n",
       " 'require': 806,\n",
       " 'stroke': 807,\n",
       " 'kay': 808,\n",
       " 'art': 809,\n",
       " 'period': 810,\n",
       " 'intend': 811,\n",
       " 'interested': 812,\n",
       " 'situation': 813,\n",
       " 'pale': 814,\n",
       " 'accept': 815,\n",
       " 'heaven': 816,\n",
       " 'design': 817,\n",
       " 'd——': 818,\n",
       " 'surface': 819,\n",
       " 'principle': 820,\n",
       " 'meant': 821,\n",
       " 'opportunity': 822,\n",
       " 'intention': 823,\n",
       " 'tomb': 824,\n",
       " 'henry': 825,\n",
       " 'tall': 826,\n",
       " 'wi': 827,\n",
       " 'sing': 828,\n",
       " 'glad': 829,\n",
       " 'nigger': 830,\n",
       " 'shook': 831,\n",
       " 'clock': 832,\n",
       " 'stir': 833,\n",
       " 'spread': 834,\n",
       " 'sorry': 835,\n",
       " 'iron': 836,\n",
       " 'quarter': 837,\n",
       " 'fifty': 838,\n",
       " 'drove': 839,\n",
       " 'handle': 840,\n",
       " 'mud': 841,\n",
       " 'yes': 842,\n",
       " 'notion': 843,\n",
       " 'beg': 844,\n",
       " 'bore': 845,\n",
       " 'tongue': 846,\n",
       " 'discover': 847,\n",
       " 'music': 848,\n",
       " 'dread': 849,\n",
       " 'glass': 850,\n",
       " 'wise': 851,\n",
       " 'public': 852,\n",
       " 'conscience': 853,\n",
       " 'cloud': 854,\n",
       " 'german': 855,\n",
       " 'afraid': 856,\n",
       " 'forth': 857,\n",
       " 'grace': 858,\n",
       " 'doctor': 859,\n",
       " 'trace': 860,\n",
       " 'paid': 861,\n",
       " 'food': 862,\n",
       " 'occur': 863,\n",
       " 'common': 864,\n",
       " 'impossible': 865,\n",
       " 'dozen': 866,\n",
       " 'merlin': 867,\n",
       " 'dry': 868,\n",
       " 'escape': 869,\n",
       " 'ought': 870,\n",
       " 'seize': 871,\n",
       " 'plain': 872,\n",
       " 'aid': 873,\n",
       " 'private': 874,\n",
       " 'number': 875,\n",
       " 'weak': 876,\n",
       " 'author': 877,\n",
       " 'excellent': 878,\n",
       " 'break': 879,\n",
       " 'surprise': 880,\n",
       " 'member': 881,\n",
       " 'mist': 882,\n",
       " 'colour': 883,\n",
       " 'mistress': 884,\n",
       " 'lyford': 885,\n",
       " 'cairo': 886,\n",
       " 'smoke': 887,\n",
       " 'grave': 888,\n",
       " 'whenever': 889,\n",
       " 'throat': 890,\n",
       " 'bury': 891,\n",
       " 'lot': 892,\n",
       " 'difference': 893,\n",
       " 'key': 894,\n",
       " 'guess': 895,\n",
       " 'reader': 896,\n",
       " 'shake': 897,\n",
       " 'energy': 898,\n",
       " 'examine': 899,\n",
       " 'drive': 900,\n",
       " 'field': 901,\n",
       " 'prisoner': 902,\n",
       " 'nearer': 903,\n",
       " 'wander': 904,\n",
       " 'accustom': 905,\n",
       " 'stream': 906,\n",
       " 'gain': 907,\n",
       " 'system': 908,\n",
       " 'forget': 909,\n",
       " 'proceed': 910,\n",
       " 'adventure': 911,\n",
       " 'wheel': 912,\n",
       " 'apparently': 913,\n",
       " 'mail': 914,\n",
       " 'seek': 915,\n",
       " 'fly': 916,\n",
       " 'date': 917,\n",
       " 'travel': 918,\n",
       " 'today': 919,\n",
       " 'shadow': 920,\n",
       " 'sake': 921,\n",
       " 'sky': 922,\n",
       " 'clearly': 923,\n",
       " 'driven': 924,\n",
       " 'distance': 925,\n",
       " 'article': 926,\n",
       " 'example': 927,\n",
       " 'description': 928,\n",
       " 'volume': 929,\n",
       " 'finish': 930,\n",
       " 'medical': 931,\n",
       " 'suggest': 932,\n",
       " 'decide': 933,\n",
       " 'usually': 934,\n",
       " 'beneath': 935,\n",
       " 'perceive': 936,\n",
       " 'visitor': 937,\n",
       " 'vessel': 938,\n",
       " 'supply': 939,\n",
       " 'teach': 940,\n",
       " 'respectable': 941,\n",
       " 'dignity': 942,\n",
       " 'twice': 943,\n",
       " 'fond': 944,\n",
       " 'honour': 945,\n",
       " 'stelling': 946,\n",
       " 'straight': 947,\n",
       " 'considerable': 948,\n",
       " 'confidence': 949,\n",
       " 'damp': 950,\n",
       " 'belong': 951,\n",
       " 'pretend': 952,\n",
       " 'charge': 953,\n",
       " 'gun': 954,\n",
       " 'spite': 955,\n",
       " 'drink': 956,\n",
       " 'dreadful': 957,\n",
       " 'chin': 958,\n",
       " 'rattle': 959,\n",
       " 'judgment': 960,\n",
       " 'value': 961,\n",
       " 'summer': 962,\n",
       " 'ate': 963,\n",
       " 'cat': 964,\n",
       " 'existence': 965,\n",
       " 'repeat': 966,\n",
       " 'generally': 967,\n",
       " 'twenty': 968,\n",
       " 'march': 969,\n",
       " 'mary': 970,\n",
       " 'temper': 971,\n",
       " 'mental': 972,\n",
       " 'nay': 973,\n",
       " 'grass': 974,\n",
       " 'merit': 975,\n",
       " 'noble': 976,\n",
       " 'century': 977,\n",
       " 'immediate': 978,\n",
       " 'render': 979,\n",
       " 'single': 980,\n",
       " 'christian': 981,\n",
       " 'countenance': 982,\n",
       " 'ship': 983,\n",
       " 'convince': 984,\n",
       " 'aware': 985,\n",
       " 'degree': 986,\n",
       " 'amount': 987,\n",
       " 'push': 988,\n",
       " 'propose': 989,\n",
       " 'personage': 990,\n",
       " 'butter': 991,\n",
       " 'hotel': 992,\n",
       " 'building': 993,\n",
       " 'apply': 994,\n",
       " 'broken': 995,\n",
       " 'admit': 996,\n",
       " 'basket': 997,\n",
       " 'skin': 998,\n",
       " 'bottle': 999,\n",
       " 'talent': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTxuijiE_-yM",
    "outputId": "1e16307b-b489-46ff-fddc-fdc05a6a49f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37165/4208752809.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences = np.array(sequences)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    sequences, label, test_size=0.3, random_state=42)\n",
    "trainX = np.array(trainX)\n",
    "testX = np.array(testX)\n",
    "trainY = np.array(trainY)\n",
    "testY = np.array(testY)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)\n",
    "\n",
    "# trainX.shape, testX.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3ykWyt8AQHn",
    "outputId": "607d7b0f-2471-48fe-928d-6aa8fd7024be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 100)\n",
      "(300, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 100\n",
    "#transforms a list (of length num_samples) of sequences (lists of integers) \n",
    "#into a 2D Numpy array of shape (num_samples, num_timesteps) num_timesteps is the maxlen argument.\n",
    "\n",
    "train_X_pad = pad_sequences(trainX, maxlen = max_len, dtype='int32')\n",
    "test_X_pad = pad_sequences(testX, maxlen = max_len, dtype='int32')\n",
    "\n",
    "print(train_X_pad.shape)\n",
    "print(test_X_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePA_jmq1Ah-3",
    "outputId": "02678b51-f520-45ed-b246-6efd38042225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "luSRMsQ7AX-7"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "def generate_model(vocab_size, max_len, embedding_size): #dropout):\n",
    "\n",
    "    _input = Input(max_len)\n",
    "\n",
    "    x = Embedding(input_dim = vocab_size, output_dim = embedding_size) (_input)\n",
    "\n",
    "    x = LSTM(50)(x)\n",
    "\n",
    "    output = Dense(7, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs= [_input], outputs = [output])  \n",
    "    #dropout = layers(Dropout(0.5))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RakgypQwtqr3"
   },
   "outputs": [],
   "source": [
    "#trainY\n",
    "#dropout sloj posle LSTM, moze da se smeni embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3WapLTSAcxF",
    "outputId": "0631693f-ede7-47c6-ee75-b05bc1d84fd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 20:46:34.166292: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-03 20:46:34.167121: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-03 20:46:34.168507: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37165/2753821764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dropout=dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37165/2808777841.py\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(vocab_size, max_len, embedding_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[1;32m    643\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[1;32m    644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m     return list(_generate_zero_filled_state_for_cell(\n\u001b[0m\u001b[1;32m   2507\u001b[0m         self, inputs, batch_size, dtype))\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2985\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2987\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   3001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3003\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3004\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3005\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2998\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2819\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2820\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2866\u001b[0m           \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m           \u001b[0;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2804\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2805\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[0;32m-> 3051\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3052\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataScience/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "model = generate_model(vocab_size , max_len , embedding_size=80) #dropout=dropout\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3aWV_dmqeUw"
   },
   "outputs": [],
   "source": [
    "trainX= np.array(trainX)\n",
    "trainY= np.array(trainY)\n",
    "testX= np.array(testX)\n",
    "testY= np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMWWQ2IabD1W",
    "outputId": "cfdc8e21-59b9-4bc7-c7e6-799d2b261e7b"
   },
   "outputs": [],
   "source": [
    "#testY\n",
    "le = preprocessing.LabelEncoder()\n",
    "trainY= le.fit_transform(trainY)\n",
    "testY = le.fit_transform(testY)\n",
    "trainY.shape,testY.shape\n",
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9xrX7gwAmZV",
    "outputId": "3c1bb03e-5e0e-41e8-ed0c-32b51bfc6f02"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X_pad, trainY, epochs=10, batch_size=60, validation_data=(test_X_pad, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMmTww8hAqC0",
    "outputId": "ea848c2d-0cd8-4672-c006-7bdb68e38079"
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(test_X_pad)\n",
    "pred_test = np.argmax(pred_test,axis=1)\n",
    "#pred_test = pred_test.round()\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test))\n",
    "print(confusion_matrix(testY,pred_test))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avQQpilG9G0V"
   },
   "outputs": [],
   "source": [
    "text = data['v2']\n",
    "label = data['v1']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test , y_train, y_test = train_test_split(text, label, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6_pvAKd91Dk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "vocab_size = 11996\n",
    "\n",
    "X_train = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_train]\n",
    "X_test = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svNt_oB3-EOk"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5N-xdPH-T2T"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model_conv1 = Sequential([\n",
    "    Embedding(vocab_size, 7, input_length=max_length),\n",
    "  Conv1D(32, 3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "  Dense(10, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9PL6FIU01-p"
   },
   "outputs": [],
   "source": [
    "model_conv1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUBcFG2x1S0T",
    "outputId": "3691dacd-0dfe-4e4f-efca-0beacb79113e"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train= le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOl_1YJj0-H4",
    "outputId": "6e0ad339-7bb0-4ffe-d93f-836f9c38233c"
   },
   "outputs": [],
   "source": [
    "history = model_conv1.fit(train_X_pad, trainY, epochs=20, validation_data=(test_X_pad, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jli-aesD27b5",
    "outputId": "09579a82-33d3-4ef2-a72b-eab976808bfb"
   },
   "outputs": [],
   "source": [
    "pred_test_conv1 = model_conv1.predict(test_X_pad)\n",
    "pred_test_conv1 = np.argmax(pred_test_conv1,axis=1)\n",
    "#pred_test = pred_test.round()\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test_conv1))\n",
    "print(confusion_matrix(testY,pred_test_conv1))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test_conv1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAL1HNus4Q67"
   },
   "source": [
    "#ExplainerDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCUSUJ2kqS4p"
   },
   "outputs": [],
   "source": [
    "df_columns = list({k: v for k, v in sorted(count_vect.vocabulary_.items(), key=lambda item: item[1])}.keys())\n",
    "X_train_df = pd.DataFrame(trainX_vec.toarray(), columns = df_columns)\n",
    "X_test_df = pd.DataFrame(testX_vec.toarray(), columns = df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "k_iwwSp7tK5Z",
    "outputId": "7dca133b-00fc-460b-f2a2-76f3758a728b"
   },
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji_5p0L78LJZ"
   },
   "outputs": [],
   "source": [
    "#X_train_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SogoZIZhtLmG",
    "outputId": "5eba211e-bff0-4991-846d-f942b32b4f7d"
   },
   "outputs": [],
   "source": [
    "model_ex = LogisticRegression()\n",
    "model_ex.fit(X_train_df,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im0YWjGswhe8"
   },
   "outputs": [],
   "source": [
    "#!! pip install explainerdashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "ZBgwjEg3tQKi",
    "outputId": "f858dbbb-1f14-4880-d2ad-967dd5f04538"
   },
   "outputs": [],
   "source": [
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "\n",
    "explainer = ClassifierExplainer(model_ex, X_test_df, testY,\n",
    "  #labels=['v1'], # defaults to ['0', '1', etc]\n",
    ")\n",
    "\n",
    "db = ExplainerDashboard(explainer,title=\"Author Classifier Explainer\",\n",
    "    shap_interaction=False,\n",
    ")\n",
    "db.run(port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zsup8NShLNW0"
   },
   "source": [
    "#Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nC_uiO2sLMsm"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xtezBzr9LiOU"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37165/4172325347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwYV1CuvMN_i"
   },
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(trainX, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(testX, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNY7mOIkMDxI"
   },
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
